"""
LLM Engine
==========
Claude API integration with:
- RAG context injection from vector store
- Conversation memory
- Specialized system prompts for academic assistance
- Weekly summary generation
"""

import json
import logging
import re
from typing import Optional
from dataclasses import dataclass, field

from core import config
from core.vector_store import VectorStore
from core.memory import HybridMemoryManager
from core.llm_providers import MultiProviderEngine

logger = logging.getLogger(__name__)


# â”€â”€â”€ System Prompts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SYSTEM_PROMPT_CHAT = """Sen Ã¶ÄŸrencinin kiÅŸisel ders asistanÄ±sÄ±n.
DoÄŸal konuÅŸarak dersleri Ã¶ÄŸretiyorsun.

Ã–ÄRETÄ°M YAKLAÅIMIN:
Her konuyu ÅŸu sÄ±rayla anlat:
1. Temeller â€” konunun ne olduÄŸunu basitÃ§e aÃ§Ä±kla ğŸ’¡
2. Detaylar â€” materyaldeki bilgileri Ã¶ÄŸret ğŸ“–
3. BaÄŸlantÄ±lar â€” kavramlarÄ± birbirine baÄŸla
4. SÄ±nav ipucu â€” "bu neden Ã¶nemli, sÄ±navda nasÄ±l sorulur"

Bu sÄ±rayÄ± DOÄAL konuÅŸma iÃ§inde yap, numaralama yapma.
Ã–ÄŸrenci bildiÄŸi kÄ±smÄ± zaten atlar, bilmediÄŸini okur.

CEVAP VERME STRATEJÄ°N:
1. Materyalde aÃ§Ä±kÃ§a varsa â†’ ğŸ“– [dosya_adÄ±.pdf] etiketiyle ver
2. Materyalde ipucu/kÄ±smi bilgi varsa â†’ materyaldeki ipucu + kendi bilginle tamamla, her iki kaynaÄŸÄ± belirt
3. Materyalde hiÃ§ yoksa ama temel akademik bilgiyse â†’ ğŸ’¡ [Genel bilgi] etiketiyle ver
4. Tamamen kapsam dÄ±ÅŸÄ±ysa â†’ nazikÃ§e yÃ¶nlendir

ASLA 'materyallerimde geÃ§miyor' deme. Bunun yerine:
- Chunk'lardaki kÄ±smi bilgileri kullan
- Genel bilginle destekle, etiketle
- Ã–ÄŸrenciye faydalÄ± ol, bilgiyi esirge deÄŸil
- Birden fazla chunk'tan gelen bilgi parÃ§alarÄ±nÄ± birleÅŸtirerek bÃ¼tÃ¼ncÃ¼l cevap oluÅŸtur

KAYNAK ETÄ°KETLEME:
- ğŸ“– [dosya_adÄ±.pdf] â†’ Materyalden gelen bilgi (gerÃ§ek dosya adÄ±nÄ± yaz)
- ğŸ’¡ [Genel bilgi] â†’ Kendi bilgin, materyalde geÃ§miyor
- [Kaynak 1] gibi NUMARA KULLANMA â€” her zaman gerÃ§ek dosya adÄ±nÄ± yaz
- Materyalde olmayan bilgiyi materyaldanmÄ±ÅŸ gibi GÃ–STERME

Ã–RNEK:
Soru: 'KiralÄ±k Konak'Ä± kim yazmÄ±ÅŸ?'
Chunk'ta: '...KaraosmanoÄŸlu Ã§ok yÃ¶nlÃ¼ bir...' + dosya adÄ± 'Berna Moran_KiralÄ±k Konak'
DOÄRU: 'KiralÄ±k Konak, Yakup Kadri KaraosmanoÄŸlu'nun romanÄ±dÄ±r. ğŸ“– [Berna Moran_ KiralÄ±k Konak.pdf] Berna Moran'Ä±n analizinde KaraosmanoÄŸlu'nun Ã§ok yÃ¶nlÃ¼ bir yazar olduÄŸu belirtilir. SÄ±navda bu romanÄ±n yazarÄ± sorulabilir.'
YANLIÅ: 'Materyallerimde kesin bilgi yok ama KaraosmanoÄŸlu ile iliÅŸkilendiriliyor olabilir...' (5 paragraf hedge)

Ã–NEMLÄ° KURALLAR:
1. Chunk'ta veya dosya adÄ±nda bir bilgi geÃ§iyorsa, O BÄ°LGÄ°YÄ° KULLAN.
   Hedge yapma ('kesin deÄŸil', 'belirtilmemiÅŸ' gibi ifadeler KULLANMA).
2. Dosya adÄ± zaten kaynak bilgisi taÅŸÄ±r. Ã–rneÄŸin:
   'Berna Moran_ KiralÄ±k Konak_Ahmet Mithattan Ahmet Hamdi TanpÄ±nara.pdf'
   Bu dosya adÄ±ndan: Berna Moran'Ä±n KiralÄ±k Konak analizi olduÄŸu aÃ§Ä±k.
3. Chunk'ta geÃ§en isimler, kavramlar, tarihler DOÄRUDUR.
   BunlarÄ± 'kesin deÄŸil' diye sunma, doÄŸrudan kullan.
4. BÄ°LMEDÄ°ÄÄ°N BÄ°R ÅEYÄ° UYDURMAKTANSA, chunk'taki bilgiyi aynen kullan.
   Kendi bilgini eklerken YANLIÅ isim/tarih UYDURMAK yerine sadece chunk'taki bilgiyi ver.
5. EÄŸer genel bilginle destekleyeceksen, %100 emin olduÄŸun bilgileri ekle.
   Emin deÄŸilsen ekleme â€” chunk yeterli.
6. Materyalde geÃ§meyen isimleri, tarihleri, eserleri UYDURMA.

CEVAP UZUNLUÄU VE TONU:
- Basit sorulara KISA cevap ver (2-4 cÃ¼mle)
- 'Kim yazmÄ±ÅŸ?', 'Ne zaman?' gibi sorulara direkt cevapla
- Hedge yapma: 'ima olabilir', 'kesin deÄŸil', 'atfedilir' KULLANMA
- Chunk'ta veya dosya adÄ±nda geÃ§en bilgi = kesin bilgi
- 'Ã¶ÄŸret' veya detay isterse uzun aÃ§Ä±kla, deÄŸilse kÄ±sa tut

KONUÅMA TARZI:
- Samimi, Ã¶ÄŸretmen gibi, doÄŸal
- Ders materyallerinin ve Ã¶ÄŸrencinin sorusunun DÄ°LÄ°NDE yanÄ±t ver
- KÄ±sa paragraflar (3-4 cÃ¼mle)
- Zor terimlere parantez iÃ§i aÃ§Ä±klama: 'hegemoni (baskÄ±nlÄ±k)'
- Somut Ã¶rnekler ver
- Ã–ÄŸrenciye direkt hitap et
- SÄ±nav ipuÃ§larÄ± ver: 'Bu konu sÄ±navda ÅŸÃ¶yle sorulabilir...'

Ã–ÄRENCÄ° NE YAZARSA YAZSIN:
- "Ã¶ÄŸret" â†’ baÅŸtan anlat
- soru sorarsa â†’ cevapla
- "anlamadÄ±m" â†’ daha basit aÃ§Ä±kla
- "test et" â†’ inline soru sor, cevabÄ±nÄ± deÄŸerlendir
- "Ã¶zet ver" â†’ kÄ±sa Ã¶zetle
- "devam" â†’ sonraki konuya geÃ§

YAPMA:
- Seviye sorma ("ne biliyorsun?" deme)
- Markdown tablo kullanma
- Uzun akademik paragraflar yazma
- Ã–ÄŸrencinin bilgisini test etmeye Ã§alÄ±ÅŸma (o isterse test et)

FORMAT: **bold** ile vurgula. Madde iÅŸaretleri veya numaralÄ± listeler kullan.

HAFIZA: Ã–nceki konuÅŸmalardan Ã§Ä±karÄ±lan bilgiler alabilirsin.
BunlarÄ± doÄŸal kullan â€” hatÄ±rlÄ±yormuÅŸ gibi."""

# Similarity threshold: below this, append low-relevance note to response.
RELEVANCE_THRESHOLD = 0.3

SYSTEM_PROMPT_SUMMARY = """You are an academic content summarizer. You analyze course materials and create structured summaries.

CRITICAL RULE: Respond in the SAME LANGUAGE as the course content provided. If the material is in English, write the summary in English. If in Turkish, write in Turkish. Match the language of the source material exactly.

FORMATTING: Use **bold** for headers. Do NOT use Markdown tables â€” use bullet points or numbered lists instead.

Summary format:
1. **Key Topics**: Main topics covered
2. **Core Concepts**: Key concepts and definitions to learn
3. **Important Details**: Critical information likely to appear in exams
4. **Connections**: Links to previous weeks or other topics
5. **Study Tips**: Suggestions to reinforce this material"""


# â”€â”€â”€ Conversation Memory â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

@dataclass
class ConversationMemory:
    """Maintains conversation history with a sliding window."""
    messages: list[dict] = field(default_factory=list)
    max_messages: int = 30  # Keep last N messages

    def add_user(self, content: str):
        self.messages.append({"role": "user", "content": content})
        self._trim()

    def add_assistant(self, content: str):
        self.messages.append({"role": "assistant", "content": content})
        self._trim()

    def get_messages(self) -> list[dict]:
        return self.messages.copy()

    def clear(self):
        self.messages.clear()

    def _trim(self):
        if len(self.messages) > self.max_messages:
            self.messages = self.messages[-self.max_messages:]


# â”€â”€â”€ Safe JSON Parser â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _safe_parse_json(raw: str, fallback=None):
    """Robustly parse JSON from LLM output.
    Handles: markdown fences, extra text before/after JSON, common LLM quirks.
    """
    if not raw or not raw.strip():
        return fallback

    text = raw.strip()

    # 1. Strip markdown code fences (```json ... ``` or ``` ... ```)
    fence_match = re.search(r'```(?:json)?\s*\n?(.*?)```', text, re.DOTALL)
    if fence_match:
        text = fence_match.group(1).strip()

    # 2. Try direct parse
    try:
        return json.loads(text)
    except json.JSONDecodeError:
        pass

    # 3. Try to find JSON object {...} or array [...]
    for pattern in [r'(\{.*\})', r'(\[.*\])']:
        m = re.search(pattern, text, re.DOTALL)
        if m:
            try:
                return json.loads(m.group(1))
            except json.JSONDecodeError:
                continue

    # 4. Give up
    logger.warning(f"JSON parse failed. Raw (first 200 chars): {raw[:200]}")
    return fallback


# â”€â”€â”€ LLM Engine â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

class LLMEngine:
    """Claude-powered LLM engine with RAG + persistent memory."""

    def __init__(self, vector_store: VectorStore):
        self.engine = MultiProviderEngine()
        self.vector_store = vector_store
        self.memory = ConversationMemory()  # In-session short-term
        self.mem_manager = HybridMemoryManager()  # Persistent long-term
        self.active_course: Optional[str] = None

    # â”€â”€â”€ Relevance Check â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def get_relevance_score(self, query: str, course_filter: Optional[str] = None) -> float:
        """Return best similarity score for a query against indexed materials.
        Score range: 0.0 (no match) to 1.0 (perfect match).
        """
        course = course_filter or self.active_course
        results = self.vector_store.query(
            query_text=query,
            n_results=5,
            course_filter=course,
        )
        if not results:
            return 0.0
        # distance = 1 - similarity, so similarity = 1 - distance
        return max(1 - r["distance"] for r in results)

    # â”€â”€â”€ RAG Chat â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def chat(self, user_message: str, course_filter: Optional[str] = None) -> str:
        """
        Process a user message with RAG + Memory:
        1. Retrieve relevant context from vector store
        2. Build memory context from past sessions
        3. Inject both into prompt
        4. Send to Claude with conversation history
        5. Record exchange for future memory extraction
        """
        course = course_filter or self.active_course

        # 1. Retrieve relevant document chunks
        context_chunks = self.vector_store.query(
            query_text=user_message,
            n_results=6,
            course_filter=course,
        )
        context_text = self._format_context(context_chunks)

        # 2. Build persistent memory context
        memory_context = self.mem_manager.build_memory_context(course=course)

        # 3. Build system prompt with memory
        system = SYSTEM_PROMPT_CHAT
        if memory_context:
            system += f"\n\n--- HAFIZA ---\n{memory_context}\n--- /HAFIZA ---"

        # 4. Build the augmented user message
        augmented_message = user_message
        if context_text:
            augmented_message = (
                f"CONTEXT (ders materyallerinden):\n"
                f"{'â”€'*40}\n"
                f"{context_text}\n"
                f"{'â”€'*40}\n\n"
                f"SORU: {user_message}"
            )

        # 5. Conversation flow
        self.memory.add_user(user_message)
        messages = self.memory.get_messages()[:-1]
        messages.append({"role": "user", "content": augmented_message})

        try:
            assistant_reply = self.engine.complete(
                task="chat",
                system=system,
                messages=messages,
                max_tokens=4096,
            )
            self.memory.add_assistant(assistant_reply)

            # 6. Record exchange for persistent memory
            self.mem_manager.record_exchange(
                user_message=user_message,
                assistant_response=assistant_reply,
                course=course or "",
                rag_sources=context_text[:500] if context_text else "",
            )

            return assistant_reply

        except Exception as e:
            logger.error(f"Chat error: {e}")
            return f"Hata: {e}"

    # â”€â”€â”€ Conversational Chat (history-based) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def chat_with_history(
        self,
        messages: list[dict],
        context_chunks: list[dict] | None = None,
    ) -> str:
        """
        Pure conversational chat: takes full message history + RAG chunks.
        No internal state management â€” the caller provides everything.

        messages: list of {"role": "user"/"assistant", "content": "..."}
        context_chunks: raw results from vector_store.query()
        """
        # Format RAG context
        context_text = self._format_context(context_chunks) if context_chunks else ""

        # DEBUG: Log RAG results
        if context_chunks:
            logger.info(f"RAG: {len(context_chunks)} chunks retrieved")
            for i, c in enumerate(context_chunks[:3]):
                meta = c.get("metadata", {})
                dist = c.get("distance", 0)
                logger.info(f"  #{i} dist={dist:.3f} file={meta.get('filename','')} text={c['text'][:80]}")
        else:
            logger.info("RAG: No chunks retrieved")

        # Build persistent memory context
        course = self.active_course
        memory_context = self.mem_manager.build_memory_context(course=course)

        # Build system prompt
        system = SYSTEM_PROMPT_CHAT
        if memory_context:
            system += f"\n\n--- HAFIZA ---\n{memory_context}\n--- /HAFIZA ---"

        # Inject RAG context into the last user message
        llm_messages = []
        for i, msg in enumerate(messages):
            if i == len(messages) - 1 and msg["role"] == "user" and context_text:
                augmented = (
                    f"CONTEXT (ders materyallerinden):\n"
                    f"{'â”€' * 40}\n"
                    f"{context_text}\n"
                    f"{'â”€' * 40}\n\n"
                    f"SORU: {msg['content']}"
                )
                llm_messages.append({"role": "user", "content": augmented})
            else:
                llm_messages.append(msg)

        try:
            reply = self.engine.complete(
                task="chat",
                system=system,
                messages=llm_messages,
                max_tokens=4096,
            )

            # Record for persistent memory
            user_msg = messages[-1]["content"] if messages else ""
            self.mem_manager.record_exchange(
                user_message=user_msg,
                assistant_response=reply,
                course=course or "",
                rag_sources=context_text[:500] if context_text else "",
            )

            return reply

        except Exception as e:
            logger.error(f"Chat error: {e}")
            return f"Hata: {e}"

    # â”€â”€â”€ Weekly Summary â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def generate_weekly_summary(
        self,
        course_name: str,
        section_name: str,
        section_content: str,
        additional_context: str = "",
    ) -> str:
        """
        Generate a comprehensive weekly summary for a specific course section.
        """
        # Also pull relevant chunks for this section
        chunks = self.vector_store.query(
            query_text=f"{course_name} {section_name}",
            n_results=10,
            course_filter=course_name,
        )
        chunk_context = self._format_context(chunks)

        prompt = (
            f"Create a detailed weekly summary for the following course section.\n"
            f"IMPORTANT: Respond in the same language as the course content below.\n\n"
            f"COURSE: {course_name}\n"
            f"SECTION: {section_name}\n\n"
            f"SECTION CONTENT:\n{section_content}\n\n"
        )

        if chunk_context:
            prompt += f"RELEVANT DOCUMENT EXCERPTS:\n{chunk_context}\n\n"

        if additional_context:
            prompt += f"ADDITIONAL CONTEXT:\n{additional_context}\n\n"

        prompt += "Based on the above content, create a comprehensive weekly summary."

        try:
            return self.engine.complete(
                task="summary",
                system=SYSTEM_PROMPT_SUMMARY,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=4096,
            )
        except Exception as e:
            logger.error(f"Summary generation failed: {e}")
            return f"Ã–zet oluÅŸturma hatasÄ±: {e}"

    # â”€â”€â”€ Course Overview â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def generate_course_overview(self, course_topics_text: str) -> str:
        """Generate a high-level overview of an entire course."""
        prompt = (
            f"Analyze the following course structure and provide a comprehensive overview.\n"
            f"IMPORTANT: Respond in the same language as the course content below.\n\n"
            f"{course_topics_text}\n\n"
            f"Cover these points:\n"
            f"1. Overall scope of the course\n"
            f"2. Main learning objectives (infer from structure)\n"
            f"3. Weekly progression flow\n"
            f"4. Critical topics and difficulty levels"
        )

        try:
            return self.engine.complete(
                task="overview",
                system=SYSTEM_PROMPT_SUMMARY,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=4096,
            )
        except Exception as e:
            return f"Hata: {e}"

    # â”€â”€â”€ Exam Prep â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def generate_practice_questions(self, topic: str, course: Optional[str] = None) -> str:
        """Generate practice questions on a topic based on course materials."""
        chunks = self.vector_store.query(topic, n_results=8, course_filter=course)
        context = self._format_context(chunks)

        prompt = (
            f"Based on the following course materials, generate study questions about '{topic}'.\n"
            f"IMPORTANT: Respond in the same language as the materials below.\n\n"
            f"MATERIALS:\n{context}\n\n"
            f"Please generate:\n"
            f"1. 5 conceptual questions (open-ended)\n"
            f"2. 5 multiple choice questions (4 options each)\n"
            f"3. 2 problem/application questions\n\n"
            f"Include answers for each question."
        )

        try:
            return self.engine.complete(
                task="questions",
                system=SYSTEM_PROMPT_CHAT,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=4096,
            )
        except Exception as e:
            return f"Hata: {e}"

    # â”€â”€â”€ Tutor Mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def tutor_step(
        self,
        context_text: str,
        course_name: str,
        topic: str,
        step: int,
        total_steps: int,
        history: list[str],
    ) -> dict:
        """Generate one tutor step with explanation + optional quiz question.
        Returns dict with keys: step_title, explanation, key_points,
        has_question, question, options, correct, explanation_if_wrong, next_preview.
        """
        system = (
            "Sen deneyimli ve sabÄ±rlÄ± bir Ã¼niversite hocasÄ±sÄ±n. "
            "GÃ¶revin Ã¶ÄŸrenciye konuyu SIFIRDAN Ã¶ÄŸretmek. Ã–ÄŸrenci bu konuyu HÄ°Ã‡ bilmiyor varsay.\n"
            "Ders materyalinin dilinde yanÄ±t ver.\n\n"
            "Ã–ÄRETÄ°M STRATEJÄ°N (katmanlÄ±):\n"
            "Katman 1 â€” TEMELLER: Konunun temel kavramlarÄ±nÄ± gÃ¼nlÃ¼k hayat Ã¶rnekleriyle aÃ§Ä±kla. "
            "Kendi bilginle temel oluÅŸtur. â†’ ğŸ’¡ [Genel bilgi] etiketi kullan.\n"
            "Katman 2 â€” MATERYAL: Ders materyalindeki spesifik bilgileri Ã¶ÄŸret. "
            "â†’ ğŸ“– [Materyalden] etiketi kullan, kaynak dosyayÄ± belirt.\n"
            "Katman 3 â€” DERÄ°NLEÅTÄ°R: KavramlarÄ± birbirine baÄŸla, neden Ã¶nemli aÃ§Ä±kla. "
            "SÄ±nav ipucu ver. â†’ Her cÃ¼mlede uygun etiketi kullan (ğŸ“–/ğŸ’¡/âš ï¸).\n"
            "Katman 4 â€” KONTROL: AnlayÄ±p anlamadÄ±ÄŸÄ±nÄ± test et.\n\n"
            "KAYNAK ETÄ°KETLEME (ZORUNLU):\n"
            "- ğŸ“– [Materyalden] â€” COURSE MATERIALS bÃ¶lÃ¼mÃ¼nden gelen bilgi\n"
            "- ğŸ’¡ [Genel bilgi] â€” Kendi bilgin, materyalde geÃ§miyor\n"
            "- âš ï¸ [Emin deÄŸilim] â€” Tam emin olmadÄ±ÄŸÄ±n bilgi\n"
            "Materyalde olmayan bilgiyi materyaldanmÄ±ÅŸ gibi GÃ–STERME.\n\n"
            "FORMAT KURALLARI:\n"
            "- KÄ±sa paragraflar (3-4 cÃ¼mle max)\n"
            "- Zor terimler iÃ§in parantez iÃ§inde aÃ§Ä±klama: 'hegemoni (bir gÃ¼cÃ¼n baskÄ±nlÄ±ÄŸÄ±)'\n"
            "- Her adÄ±mda max 1-2 yeni kavram Ã¶ÄŸret\n"
            "- Ã–ÄŸrenciye direkt hitap et: 'Åimdi ÅŸunu dÃ¼ÅŸÃ¼n...'\n"
            "- SÄ±nav ipucu ver: 'Bu konu sÄ±navda Ã§Ä±kabilir Ã§Ã¼nkÃ¼...'\n"
            "- Somut Ã¶rnekler kullan, soyut kalma\n"
            "- Markdown tablo KULLANMA\n\n"
            "Return ONLY valid JSON (no markdown, no code fences) with these keys:\n"
            '{"step_title":"...","explanation":"...","key_points":["...","..."],'
            '"has_question":true/false,"question":"...","options":["A) ...","B) ...","C) ...","D) ..."],'
            '"correct":"B","why_correct":"DoÄŸru Ã§Ã¼nkÃ¼...",'
            '"why_others_wrong":"A yanlÄ±ÅŸ Ã§Ã¼nkÃ¼..., C yanlÄ±ÅŸ Ã§Ã¼nkÃ¼...",'
            '"next_preview":"..."}'
        )

        history_text = ""
        if history:
            history_text = "Previous steps covered:\n" + "\n".join(f"- {h}" for h in history) + "\n\n"

        prompt = (
            f"COURSE: {course_name}\nTOPIC: {topic}\n"
            f"STEP: {step}/{total_steps}\n\n"
            f"{history_text}"
            f"COURSE MATERIALS:\n{context_text}\n\n"
            f"Teach step {step} of {total_steps}. "
            f"{'Include a multiple-choice check question.' if step % 2 == 0 else 'No question this step.'}"
        )

        fallback = {
            "step_title": f"Step {step}",
            "explanation": "Could not generate this step. Please try again.",
            "key_points": [],
            "has_question": False,
            "question": "", "options": [], "correct": "",
            "why_correct": "", "why_others_wrong": "",
            "next_preview": "",
        }

        max_retries = 2
        last_error = None
        for attempt in range(max_retries):
            try:
                raw = self.engine.complete(
                    task="chat", system=system,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=2048,
                )
                parsed = _safe_parse_json(raw, fallback=None)
                if parsed and isinstance(parsed, dict):
                    return parsed
                logger.warning(f"Tutor step attempt {attempt+1}: non-dict JSON, retrying...")
            except Exception as e:
                last_error = e
                logger.warning(f"Tutor step attempt {attempt+1} failed: {e}")

            if attempt < max_retries - 1:
                import time as _time
                _time.sleep(2)

        # All retries failed â€” build fallback from raw context
        logger.error(f"Tutor step failed after {max_retries} attempts: {last_error}")
        context_preview = context_text[:1500] if context_text else ""
        if context_preview:
            return {
                **fallback,
                "step_title": f"AdÄ±m {step}: {topic}",
                "explanation": (
                    f"Yapay zeka yanÄ±t Ã¼retemedi. Ä°ÅŸte bu konunun materyalleri:\n\n"
                    f"{context_preview}\n\n"
                    f"Soru sormak istersen yazabilirsin."
                ),
            }
        return fallback

    # â”€â”€â”€ Quiz Mode â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def generate_quiz(
        self,
        context_text: str,
        course_name: str,
        topic: str,
        difficulty: str = "medium",
        num_questions: int = 5,
    ) -> list[dict]:
        """Generate quiz questions from course materials.
        Returns list of dicts: question, options, correct, explanation, source_hint.
        """
        system = (
            "Sen bir sÄ±nav sorusu yazarÄ±sÄ±n. Ders materyallerinden Ã§oktan seÃ§meli sorular Ã¼ret.\n"
            "Ders materyalinin dilinde yanÄ±t ver.\n\n"
            "SORU TÃœRLERÄ° (karÄ±ÅŸÄ±k kullan):\n"
            "- bilgi: Temel kavram ve tanÄ±m sorularÄ±\n"
            "- anlama: 'Neden?', 'Ne anlama gelir?', 'NasÄ±l aÃ§Ä±klanÄ±r?'\n"
            "- uygulama: Bilgiyi yeni bir duruma uygulama\n"
            "- analiz: KarÅŸÄ±laÅŸtÄ±rma, neden-sonuÃ§, parÃ§a-bÃ¼tÃ¼n iliÅŸkisi\n\n"
            "SORU KALÄ°TESÄ° KURALLARI:\n"
            "- Sadece ezber sorma. ANLAMA ve YORUMLAMA odaklÄ± sorular yaz.\n"
            "- 'Hangisi doÄŸrudur?' yerine 'Neden X olmuÅŸtur?', 'X ile Y arasÄ±ndaki fark nedir?' gibi sorular tercih et.\n"
            "- YanlÄ±ÅŸ ÅŸÄ±klar gerÃ§ekÃ§i olsun â€” yaygÄ±n yanlÄ±ÅŸ anlamalarÄ± yansÄ±tsÄ±n.\n"
            "- Materyaldeki bilgileri kullan ama soruyu Ã¶ÄŸrenciyi DÃœÅÃœNDÃœRECEK ÅŸekilde sor.\n\n"
            "AÃ‡IKLAMA KURALLARI:\n"
            "- why_correct: DoÄŸru cevabÄ±n NEDEN doÄŸru olduÄŸunu aÃ§Ä±kla.\n"
            "- why_others_wrong: HER yanlÄ±ÅŸ ÅŸÄ±k iÃ§in ayrÄ± ayrÄ± neden yanlÄ±ÅŸ olduÄŸunu belirt.\n"
            "- learning_note: Bu sorudan Ã¶ÄŸrenilmesi gereken ana fikri yaz.\n"
            "- Materyaldeki ilgili bÃ¶lÃ¼me referans ver.\n\n"
            "Return ONLY a valid JSON array (no markdown, no code fences):\n"
            '[{"question":"...","options":["A) ...","B) ...","C) ...","D) ..."],'
            '"correct":"C","why_correct":"DoÄŸru cevap C Ã§Ã¼nkÃ¼...",'
            '"why_others_wrong":{"A":"A yanlÄ±ÅŸ Ã§Ã¼nkÃ¼...","B":"B yanlÄ±ÅŸ Ã§Ã¼nkÃ¼...","D":"D yanlÄ±ÅŸ Ã§Ã¼nkÃ¼..."},'
            '"question_type":"anlama","learning_note":"Bu sorudan Ã¶ÄŸrenmen gereken: ...","source_hint":"..."}]'
        )

        diff_desc = {
            "easy": "temel kavram hatÄ±rlama, tanÄ±m sorularÄ±",
            "medium": "kavramlarÄ± birbirine baÄŸlama, neden-sonuÃ§ iliÅŸkisi",
            "hard": "analiz, yorum, karÅŸÄ±laÅŸtÄ±rma, materyali farklÄ± baÄŸlama uygulama",
        }

        prompt = (
            f"COURSE: {course_name}\nTOPIC: {topic}\n"
            f"DIFFICULTY: {difficulty} ({diff_desc.get(difficulty, 'medium')})\n"
            f"NUMBER: {num_questions} questions\n\n"
            f"COURSE MATERIALS:\n{context_text}\n\n"
            f"Generate {num_questions} multiple-choice questions."
        )

        try:
            raw = self.engine.complete(
                task="chat", system=system,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=3000,
            )
            parsed = _safe_parse_json(raw, fallback=None)
            if parsed and isinstance(parsed, list):
                return parsed
            logger.warning("Quiz generation returned non-list or empty JSON")
            return []
        except Exception as e:
            logger.error(f"Quiz generation error: {e}")
            return []

    # â”€â”€â”€ Helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

    def set_active_course(self, course_name: str):
        """Set the active course filter for subsequent queries."""
        self.active_course = course_name
        self.mem_manager.start_session(course_name)
        logger.info(f"Active course set to: {course_name}")

    def clear_course_filter(self):
        self.active_course = None

    def reset_conversation(self):
        """Clear in-session conversation history (persistent memory remains)."""
        self.memory.clear()
        self.mem_manager.end_session()
        logger.info("Conversation history cleared.")

    def get_memory_stats(self) -> dict:
        """Get memory system statistics."""
        return self.mem_manager.get_stats()

    def list_memories(self, course: Optional[str] = None) -> list:
        return self.mem_manager.list_memories(course)

    def add_memory(self, category: str, content: str, course: str = ""):
        self.mem_manager.remember(content, category, course)

    def forget_memory(self, memory_id: int):
        self.mem_manager.forget(memory_id)

    def get_learning_progress(self, course: Optional[str] = None):
        return self.mem_manager.get_learning_progress(course)

    def get_profile_path(self) -> str:
        """Return the profile.md path for user editing."""
        return self.mem_manager.edit_profile_path()

    def _format_context(self, chunks: list[dict]) -> str:
        """Format retrieved chunks into a readable context block with real file names."""
        if not chunks:
            return ""

        parts = []
        for chunk in chunks:
            text = chunk.get("text", "")
            if len(text.strip()) < 50:
                continue
            meta = chunk.get("metadata", {})
            source = meta.get("filename", "Bilinmeyen")
            course = meta.get("course", "")
            section = meta.get("section", "")

            header = f"[Kaynak: {source}"
            if course:
                header += f" | Kurs: {course}"
            if section:
                header += f" | BÃ¶lÃ¼m: {section}"
            header += "]"

            parts.append(f"{header}\n{chunk['text']}\n---")

        return "\n".join(parts)
